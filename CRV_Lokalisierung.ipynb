{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer and Robot Vision project (Teil 1: Die Lokalisierung)\n",
    "\n",
    "In diesem Notebook geht es darum, wie wir Geschwindigkeitsschilder lokalisieren und klassifizieren können. Anders als üblich setzen wir dabei nicht auf herkömmliche Bildverarbeitung, sondern trainieren KI-Modelle, die diese Aufgaben eigenständig übernehmen können.\n",
    "\n",
    "Die Idee dahinter ist, das Projekt in zwei Teile aufzuteilen, die unabhängig voneinander funktionieren können. Ein Modell wird speziell darauf trainiert, die Schilder im Bild zu lokalisieren, während ein separates Modell nur für die Klassifizierung zuständig ist. Dieses Notebook konzentriert sich auf die Lokalisierung und gibt Einblicke in die Methoden und Erkenntnisse, die dabei zum Einsatz kommen.\n",
    "\n",
    "---\n",
    "\n",
    "**Author:**\n",
    "\n",
    "Dominik Bücher, Hochschuhle Heilbronn, Automotive System Engineering Master | dbuecher@stud.hs-heilbronn.de\n",
    "\n",
    "Aaron Kiani, Hochschuhle Heilbronn, Mechatronik und Robotik Master | akiani@stud.hs-heilbronn.de\n",
    "\n",
    "\n",
    "**Professor:**\n",
    "\n",
    "Prof. Dr. Dieter Maier \n",
    "\n",
    "dieter.maier@hs-heilbronn.de\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "#### 1. [Einleitung](#Introduction)\n",
    "#### 2. [Importieren der Bibliotheken](#Import)\n",
    "#### 3. [Erstellen eines eigenen Datensatzes](#Dataset)\n",
    "#### 4. [Vorstellen der verwendeten Modelle](#modelle)\n",
    "#### 5. [Training](#Training)\n",
    "#### 6. [Evaluation](#Evaluation)\n",
    "#### 7. [Testing](#Testing)\n",
    "#### 8. [Diskussion der Ergebnisse](#Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Einleitung <a id=\"Introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im kommenden Kapitel wird zunächst die Struktur dieses Notebooks vorgestellt sowie das übergeordnete Ziel dieser Aufgabe erläutert.\n",
    "\n",
    "Wie bereits zu Beginn des Notebooks erwähnt, liegt der Fokus dieses Abschnitts auf der Lokalisierung von Geschwindigkeitsschilder. Hierbei stehen verschiedene Ansätze zur Verfügung, wobei wir uns für einen eher klassischen Weg entschieden haben: die Verwendung von Cascading-Modellen.\n",
    "\n",
    "Cascading-Modelle repräsentieren eine spezielle Klasse von Klassifikationsmodellen, die nicht nur auf dem etablierten \"Haar-like features\"-Ansatz basieren, sondern auch Local Binary Pattern (LBP) einbeziehen. Diese kombinierte Methode vereint die Effizienz der Haar-ähnlichen Merkmale mit der präzisen Texturerkennung von LBP, was eine vielseitige Lösung für die Objekterkennung in Bildern bietet. Der Begriff \"cascading\" (kaskadierend) leitet sich davon ab, dass diese Modelle hierarchisch angeordnete Kaskaden von Klassifikatoren nutzen.\n",
    "\n",
    "Die Grundidee der Cascading-Modelle besteht darin, hierarchisch angeordnete Kaskaden von Klassifikatoren zu verwenden. Jede Stufe dieser Kaskade filtert gezielt bestimmte Bildbereiche und leitet nur die vielversprechendsten Regionen an die nachfolgende Stufe weiter. Diese kaskadierende Struktur ermöglicht eine effiziente Ablehnung von irrelevanten Bereichen und konzentriert die Ressourcen auf vielversprechende Regionen, was insbesondere bei der Verarbeitung großer Bildmengen von Vorteil ist.\n",
    "\n",
    "Obwohl Cascading-Modelle ihre Bekanntheit hauptsächlich der Gesichtserkennung verdanken, eignen sie sich ebenso für die Erkennung anderer Objekte wie Geschwindigkeitsschilder oder Fahrzeuge. Der Trainingsprozess umfasst die Nutzung positiver und negativer Beispiele, um das Modell auf die spezifischen Mustererkennungsaufgaben anzupassen. Diese adaptive Anpassung ermöglicht eine präzise und effiziente Objekterkennung in verschiedenen Kontexten.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importieren der Bibliotheken <a id=\"Import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Erstellen eines eigenen Datensatzes <a id=\"Dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein Cascading-Modell erfolgreich zu trainieren, ist ein geeigneter Datensatz von entscheidender Bedeutung. Zu diesem Zweck wurden verschiedene Videos in und um Heilbronn aufgenommen. Die Aufnahmen erfolgten mit einer GoPro 11 unter den folgenden Einstellungen: 30 FPS (Bildern pro Sekunde) und einer Auflösung von 1920x1080 Pixel (HD).\n",
    "\n",
    "Die aufgenommenen Videos wurden anschließend mithilfe eines speziellen Skripts in Bilddateien umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_to_frames(video_path, output_path):\n",
    "    video_capture = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not video_capture.isOpened():\n",
    "        print(\"Fehler beim Öffnen des Videos.\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = video_capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_filename = f\"frame_{frame_count:04d}.jpg\"\n",
    "        frame_path = os.path.join(output_path, frame_filename)\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_count += 1\n",
    "    print(f\"{frame_count} Frames wurden erfolgreich extrahiert.\")\n",
    "    video_capture.release()\n",
    "\n",
    "\n",
    "video_path = r\"videos_24_11_2023\\video_speed_combined.mp4\"\n",
    "output_path = r\"videos_24_11_2023\\combined\"\n",
    "video_to_frames(video_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach der Umwandlung der Videos in Bilder ist es nun möglich, die einzelnen Bilder zu labeln, was von entscheidender Bedeutung ist, da ein Modell ohne diese Annotationen nicht trainiert werden kann.\n",
    "\n",
    "Der Kennzeichnungsprozess beinhaltete das Auffinden aller Geschwindigkeitsschilder in einem Bild und das Hinzufügen von Begrenzungsrahmen (Bounding Boxes) um diese. Es gibt verschiedene Programme, die für diese Aufgabe verwendet werden können, und wir haben uns für DarkLabel 2.4 entschieden. Dabei ist zu erwähnen das das Labeln sehr viel zeit in anspruch nehmen kann, wobei DarkLabel2.4 diese zeit zumindest etwas verkürzt hat. \n",
    "\n",
    "Nachdem alle Bilder erfolgreich gelabelt wurden, wird eine Textdatei generiert, die die Daten zu jeder einzelnen Bounding Box enthält. Diese Datei bildet die Grundlage für das spätere Training des Modells.\n",
    "\n",
    "Die Text Datei ist dabei wie folgt aufgebaut: 00000.jpg,1,795,428,50,49. Jedoch wird für das training ein Format wie dieses \"00000.jpg 1 795 428 50 49\" benötigt, bzw \"frame_3321.jpg 2 177 499 157 141 1692 577 103 93\" falls es zwei Bounding Boxen in einem Bild gibt. Zusätzlich dazu dürfen die Koordinaten nicht die Größe des Bildes überschreiten. All diese kleinen Änderungen werden in den nachfolgenden Funktionen behoben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_coordinates(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split(' ')\n",
    "            if len(parts) == 6:\n",
    "                x_coordinate = int(parts[2])\n",
    "                width = int(parts[4])\n",
    "\n",
    "                # Überprüfe, ob die Koordinaten größer als 1919 sind\n",
    "                if x_coordinate > 1919 or (x_coordinate + width) > 1919:\n",
    "                    if x_coordinate > 1919:\n",
    "                        x_coordinate = 1919 - width\n",
    "                    if (x_coordinate + width) > 1919:\n",
    "                        width = 1919 - x_coordinate\n",
    "\n",
    "                # Überprüfe, ob die Koordinaten kleiner als 1 sind\n",
    "                if x_coordinate < 1 or (x_coordinate + width) < 1:\n",
    "                    if x_coordinate < 1:\n",
    "                        x_coordinate = 1\n",
    "                    if (x_coordinate + width) < 1:\n",
    "                        width = 1 - x_coordinate\n",
    "\n",
    "                # Erstelle die aktualisierte Zeile\n",
    "                updated_line = f\"{parts[0]},{parts[1]},{x_coordinate},{parts[3]},{width},{parts[5]}\\n\"\n",
    "                outfile.write(updated_line)\n",
    "            else:\n",
    "                # Schreibe Zeilen ohne das erwartete Format unverändert\n",
    "                outfile.write(line)\n",
    "\n",
    "\n",
    "def combine_lines(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        lines = infile.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines) - 1:\n",
    "            current_line = lines[i].strip().split(';')\n",
    "            next_line = lines[i + 1].strip().split(';')\n",
    "\n",
    "            if current_line[0] == next_line[0]:\n",
    "                combined_line = [current_line[0], str(int(current_line[1]) + 1)] + [current_line[2]] + current_line[3:] + next_line[1:]\n",
    "                lines[i] = ' '.join(combined_line) + '\\n'\n",
    "                lines.pop(i + 1)\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Ersetze alle Kommas durch Leerzeichen in den verbleibenden Zeilen\n",
    "        for j in range(len(lines)):\n",
    "            lines[j] = lines[j].replace(';', ' ')\n",
    "\n",
    "        outfile.writelines(lines)\n",
    "\n",
    "\n",
    "\n",
    "adjust_coordinates(r'Dominik\\annotation_files\\train.txt', r'Dominik\\annotation_files\\train2.txt')\n",
    "combine_lines(r'Dominik\\gt copy2.txt', r'Dominik\\pos_new.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die Textdatei nun im korrekten Format vorliegt, schreiten wir zum nächsten Schritt: der Aufteilung des Datensatzes in Trainings- und Testbilder. Der folgende Code generiert zwei neue Textdateien für diese Aufteilung. Dabei werden die Daten zunächst zufällig durchmischt und anschließend aufgeteilt. Der Trainingsdatensatz umfasst dabei 95% der Bilder, während der Testdatensatz nur 5% enthält. Obwohl eine übliche Aufteilung 90 zu 10 wäre, haben wir uns aufgrund der begrenzten Anzahl von insgesamt 3726 Bildern entschieden, dem Training einen größeren Datensatzanteil zuzuweisen, wie in der folgenden Abbildung verdeutlicht wird:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Notebook_Bilder/Split_data.png\" width=\"500\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(input_datei, ausgabe_datei_1, ausgabe_datei_2, trennungsprozentsatz=5):\n",
    "    with open(input_datei, 'r') as datei:\n",
    "        zeilen = datei.readlines()\n",
    "    \n",
    "    random.shuffle(zeilen)\n",
    "    \n",
    "    trennungspunkt = int(len(zeilen) * (trennungsprozentsatz / 100))\n",
    "    ausgabe_1 = zeilen[:trennungspunkt]\n",
    "    ausgabe_2 = zeilen[trennungspunkt:]\n",
    "    \n",
    "    with open(ausgabe_datei_1, 'w') as datei_1:\n",
    "        datei_1.writelines(ausgabe_1)\n",
    "    \n",
    "    with open(ausgabe_datei_2, 'w') as datei_2:\n",
    "        datei_2.writelines(ausgabe_2)\n",
    "\n",
    "\n",
    "input_datei = r'Dominik\\annotation_files\\pos_own.txt'\n",
    "ausgabe_datei_1 = r'Dominik\\annotation_files\\test.txt'\n",
    "ausgabe_datei_2 = r'Dominik\\annotation_files\\train.txt'\n",
    "split_dataset(input_datei, ausgabe_datei_1, ausgabe_datei_2, trennungsprozentsatz=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nachdem die beiden Textdateien erstellt wurden, haben Sie die Möglichkeit, die Bilder in die entsprechenden Ordner zu verschieben. Obwohl dies nicht zwingend erforderlich ist, trägt es zur besseren Übersicht bei. Für das eigentliche Training muss die Textdatei der Trainingsdaten zwar erneut verarbeitet werden, doch dieser Schritt wird in einem späteren Kapitel ausführlich behandelt.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vorstellen der verwendeten Modelle <a id=\"modelle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
